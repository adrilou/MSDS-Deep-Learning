{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Week 5 Peer Review\n## Introduction to Project\n### What Problem am I solving\nThe purpose of this project is to explore the full capabilities of GAN's. For this project we are given data from a data file of Monet painting and another file of other photos. Since Modet has a very distinctive feel that he gives to all of his paintings, the goal of the project is to assign his \"look\" to the file of random photos. At the end of this project, I will have created a zip file that includes the random photos after they've been transformed into the style of Monet.\n### Data Description\nAs I mentioned previously, we are given two sets of data in the form of photos. The Monet photos that are included are photos of his paintings. In total, there are 300 of his paintings. The other dataset has random photos that are going to be used/converted by the GAN to be a Monet style painted photo.","metadata":{}},{"cell_type":"markdown","source":"## Exploratory Data Analysis Strategy\nExploratory Data Analysis (EDA) is essential for understanding the characteristics of the data, identifying patterns, and gaining insights that can be leveraged during model training and evaluation. Here's a good EDA strategy for this project:\n1. Load and Inspect the Data - the goal with this step is to display a few images so that I can better understand what the proper \"look\" I'm going for with my GAN\n2. Visualize the color distribution - the goal with this is to be able to see what types of colors are commonly used to paint Monets. This is also helpful because it can be difficult for the naked eye to see the real distribution of a photo when you're looking at all of the photo as a whole. A distribution of the colors in the paintings will help to see the distribution of the actual pixel values.\n3. Understand the dataset characteristics - this step will give me a high-level view of what the data looks like and will provide meta information on the files.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom tqdm.notebook import tqdm\nimport torch.optim as optim\nimport torch.nn as nn\nfrom PIL import Image\nimport torchvision\nimport shutil\nimport torch\nimport os\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid","metadata":{"execution":{"iopub.status.busy":"2024-04-22T00:32:31.669617Z","iopub.execute_input":"2024-04-22T00:32:31.670267Z","iopub.status.idle":"2024-04-22T00:32:36.277158Z","shell.execute_reply.started":"2024-04-22T00:32:31.670237Z","shell.execute_reply":"2024-04-22T00:32:36.276199Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"monet_dir = \"/kaggle/input/gan-getting-started/monet_jpg\"\nmonet_images = os.listdir(monet_dir)\nprint(\"Number of Monet paintings:\", len(monet_images))\n\nphoto_dir = \"/kaggle/input/gan-getting-started/photo_jpg\"\nphoto_images = os.listdir(photo_dir)\nprint(\"Number of photos:\", len(photo_images))\n\n# Analyze image sizes\nmonet_sizes = []\nfor img_name in monet_images:\n    img = plt.imread(os.path.join(monet_dir, img_name))\n    monet_sizes.append(img.shape)\n\nphoto_sizes = []\nfor img_name in photo_images:\n    img = plt.imread(os.path.join(photo_dir, img_name))\n    photo_sizes.append(img.shape)\n\n# Plot size distributions\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.hist(monet_sizes, bins=20, edgecolor='black')\nplt.title('Monet Image Size Distribution')\nplt.xlabel('Size')\nplt.ylabel('Frequency')\n\nplt.subplot(1, 2, 2)\nplt.hist(photo_sizes, bins=20, edgecolor='black')\nplt.title('Photo Image Size Distribution')\nplt.xlabel('Size')\nplt.ylabel('Frequency')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T02:06:03.864997Z","iopub.execute_input":"2024-04-22T02:06:03.865727Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Number of Monet paintings: 300\nNumber of photos: 7038\n","output_type":"stream"}]},{"cell_type":"code","source":"def plot_color_distribution(image_dir, title):\n    r, g, b = [], [], []\n    for img_name in os.listdir(image_dir):\n        img = plt.imread(os.path.join(image_dir, img_name))\n        r.append(np.mean(img[:,:,0]))\n        g.append(np.mean(img[:,:,1]))\n        b.append(np.mean(img[:,:,2]))\n    \n    plt.figure(figsize=(15,5))\n    plt.subplot(1, 3, 1)\n    plt.hist(r, bins=30, color='r', alpha=0.7)\n    plt.xlabel('Red Intensity')\n    plt.ylabel('Frequency')\n    plt.title(f'Red Intensity Distribution - {title}')\n\n    plt.subplot(1, 3, 2)\n    plt.hist(g, bins=30, color='g', alpha=0.7)\n    plt.xlabel('Green Intensity')\n    plt.ylabel('Frequency')\n    plt.title(f'Green Intensity Distribution - {title}')\n\n    plt.subplot(1, 3, 3)\n    plt.hist(b, bins=30, color='b', alpha=0.7)\n    plt.xlabel('Blue Intensity')\n    plt.ylabel('Frequency')\n    plt.title(f'Blue Intensity Distribution - {title}')\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_color_distribution(monet_dir, 'Monet Paintings')\nplot_color_distribution(photo_dir, 'Photos')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PaintingsDataset(Dataset):\n\n    def __init__(self, folder_path, transform=None):\n        self.folder_path = folder_path\n        self.transform = transform\n        self.images = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image_path = self.images[idx]\n        image = Image.open(image_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image\n    \n    def __get_crude_item__(self, idx):\n        image_path = self.images[idx]\n        image = Image.open(image_path).convert('RGB')\n        return image","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Architecture Description\nFor my model, I decided to use two PyTorch models for a CycleGAN: a discriminator and a generator. \n\nThe Discriminator class aims to distinguish between real images and those generated by the generator. This neural network will learn to classify images as real or fake. The discriminator uses a series of convolutional layers to downsample the image. Each block of downsampling reduces the spatial dimensions while increasing the number of channels. After several downsampling blocks, the discriminator performs upsampling. Finally, a convolutional layer is applied to produce the output.\n\nThe Generator class is responsible for generating images from input images. It learns to translate images from one domain to another. The generator takes the input image and converts it into another domain using a series of downsample and upsample blocks. The downsample blocks reduce the spatial dimensions of the input, while the upsample blocks increase the dimensions. Residual blocks help in maintaining the identity of the image during the transformation process. The output layer produces the final image.\n\n### Hyperparameter Tuning\nWhile setting up my notebook and researching other methods of building this GAN, I saw that being able to use a TPU for analysis was crucial for running each code block for model development in a reasonable amount of time. Unfortunately, when I enable the TPU, none of the blocks run and the keras library throws an error on import, so I am stuck using the GPU for this assignment.  In order to run the code below the first time, I had to wait over 2 hours for it to run, so I decided to do hyperparameter research based on what other high scoring notebooks had used and made my next attempt my best attempt. I also googled what epoch, learning rate, and batch sizes worked best for image processing and for GAN creation and actually found that higher epochs were crucial for GAN projects.\n\nFor this reason, I decided to get an epoch size of 100. This high epoch size allowed the GAN to continue learning about the image qualities and important features that are necessary for building new images in a Monet style. The batch size was able to be a smaller, more normal size at 6 since we did not have too many Monet paintings to train on. ","metadata":{}},{"cell_type":"code","source":"# Device and tensors configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nTensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n\n# Hyperparameters\nbatch_size = 6\nlr = 2e-4\nn_epoches = 100\ndisplay_epoch = 10\n\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n\n    # Data augmentation \n    #transforms.ElasticTransform(alpha=150.0),\n    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 3.)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.15, 0.15, 0.15], std=[0.5, 0.5, 0.5])\n])\n\n# Initialize datasets\nmonet_dataset = PaintingsDataset('/kaggle/input/gan-getting-started/monet_jpg', transform)\nphoto_dataset = PaintingsDataset('/kaggle/input/gan-getting-started/photo_jpg', transform)\n\n\n# Initialize data loaders\nmonet_loader = DataLoader(monet_dataset, batch_size=batch_size, shuffle=True)\nphoto_loader = DataLoader(photo_dataset, batch_size=batch_size, shuffle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        channels_img = 3\n        features_d = 64\n        \n        self.model = nn.Sequential(\n            # Input : N x 3 x 256 x 256\n            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n\n            # Downsampling blocks\n            self._block(features_d, features_d * 2, 4, 2, 1),\n            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n\n            # Upscaling blocks\n            self._conv2d_transpose(features_d * 8, features_d * 4, 3, 2, 1, 1),\n            self._conv2d_transpose(features_d * 4, features_d * 2, 3, 2, 1, 1),\n            self._conv2d_transpose(features_d * 2, features_d, 3, 2, 1, 1),\n\n            # Final convolutional layer\n            nn.Conv2d(features_d, 1, kernel_size=3, stride=2, padding=1)\n        )\n\n    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n            nn.InstanceNorm2d(out_channels, affine=True),\n            nn.LeakyReLU(0.2),\n        )\n\n    def _conv2d_transpose(self, in_channels, out_channels, kernel_size, stride, padding, output_padding):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding, bias=False),\n            nn.InstanceNorm2d(out_channels, affine=True),\n            nn.LeakyReLU(0.2),\n        )\n\n    def forward(self, x):\n        return self.model(x)\n    \n    \nclass Generator(nn.Module):\n    def __init__(self, num_residual_blocks=9):\n        super(Generator, self).__init__()\n        \n        channels_img = 3\n        features_g = 64\n        \n        # Initial convolution layer\n        self.initial = nn.Sequential(\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(channels_img, features_g, kernel_size=7, stride=1, padding=0),\n            nn.InstanceNorm2d(features_g),\n            nn.ReLU(inplace=True)\n        )\n\n        # Downsample layers\n        self.downsample_blocks = nn.Sequential(\n            self._downsample_block(features_g, features_g * 2),\n            self._downsample_block(features_g * 2, features_g * 4)\n        )\n\n        # Residual blocks\n        self.residual_blocks = nn.Sequential(\n            *[ResidualBlock(features_g * 4) for _ in range(num_residual_blocks)]\n        )\n\n        # Upsample layers\n        self.upsample_blocks = nn.Sequential(\n            self._upsample_block(features_g * 4, features_g * 2),\n            self._upsample_block(features_g * 2, features_g)\n        )\n\n        # Output layer\n        self.output = nn.Sequential(\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(features_g, channels_img, kernel_size=7, stride=1, padding=0),\n            nn.Tanh()\n        )\n\n    def _downsample_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n            nn.InstanceNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def _upsample_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.InstanceNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.initial(x)\n        x = self.downsample_blocks(x)\n        x = self.residual_blocks(x)\n        x = self.upsample_blocks(x)\n        return self.output(x)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.block = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(channels, channels, 3),\n            nn.InstanceNorm2d(channels),\n            nn.ReLU(inplace=True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(channels, channels, 3),\n            nn.InstanceNorm2d(channels)\n        )\n\n    def forward(self, x):\n        return x + self.block(x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G = Generator()\nD = Discriminator()\n\nmodels = [G, D]\nfor model in models:\n    if torch.cuda.is_available():\n        model.cuda()\n\noptimizer_G = torch.optim.RMSprop(G.parameters(), lr=lr)\noptimizer_D = torch.optim.RMSprop(D.parameters(), lr=lr)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sample_images(real_X):    \n    # turn off some learning parameters in order to evaluate model:\n    G.eval()\n    \n    # transform to tensor and plot images\n    real_pic = real_X.type(Tensor)\n    artificial_pic = G(real_X).detach()\n\n    ncols = real_X.size(0)\n    real_pic_grid = make_grid(real_pic, nrow=ncols, normalize=True)\n    artificial_pic_grid = make_grid(artificial_pic, nrow=ncols, normalize=True)\n\n    fig, axs = plt.subplots(2, 1, figsize=(20, 4))  \n\n    axs[0].imshow(real_pic_grid.permute(1, 2, 0).cpu())\n    axs[0].set_title(\"Augmented real photos from photo dataset\")\n    axs[0].axis('off')\n\n    axs[1].imshow(artificial_pic_grid.permute(1, 2, 0).cpu())\n    axs[1].set_title(\"Generated monet paintings from photos\")\n    axs[1].axis('off')\n\n\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CRITIC_ITERATIONS = 5\nWEIGHT_CLIP = 0.01\nlambda_identity = 5\nidentity_loss_function = nn.L1Loss()\n\nfor epoch in tqdm(range(n_epoches)):\n    for i, (real_photo, real_monet) in enumerate(zip(photo_loader, monet_loader)):\n\n        # Prepare the real and generated data\n        real_photo = real_photo.type(Tensor)\n        real_monet = real_monet.type(Tensor)\n\n        # Train the Critic - We wish to maximize the difference between the fake and the real images - Max(E[critic(real)] - E[critic(fake)])\n        for _ in range(CRITIC_ITERATIONS):\n            optimizer_D.zero_grad()\n\n            # Generate a batch of images\n            artificial_monet = G(real_photo)\n\n            # Critic scores for real and fake images\n            critic_real = D(real_monet).reshape(-1)\n            critic_fake = D(artificial_monet.detach()).reshape(-1)\n\n            # Calculate the loss for the critic\n            loss_critic = -(torch.mean(critic_real) - torch.mean(critic_fake))\n            loss_critic.backward(retain_graph=True)\n            optimizer_D.step()\n\n            # Weight clipping - to satisfy lipshitz constraint\n            for p in D.parameters():\n                p.data.clamp_(-WEIGHT_CLIP, WEIGHT_CLIP)\n\n        # Train the Generator - We wish to minimize \n        optimizer_G.zero_grad()\n\n        # Since we just updated D, perform another forward pass of all-fake batch through D\n        gen_fake = D(artificial_monet).reshape(-1)\n\n        # Calculate G's loss\n        loss_G_Wasserstein = -torch.mean(gen_fake)\n\n        # Calculate identity loss\n        identity_loss = identity_loss_function(artificial_monet, real_photo)\n\n        # Total generator loss\n        loss_G = loss_G_Wasserstein + lambda_identity * identity_loss\n        \n        loss_G.backward()\n        optimizer_G.step()\n        \n    if (epoch + 1) % display_epoch == 0:\n        test_real_photo, test_real_monet = next(iter(zip(photo_loader, monet_loader)))\n        sample_images(test_real_photo.type(Tensor))\n\n        print(f'Epoch {epoch + 1}/{n_epoches}')\n        print(f'Generator loss: {loss_G.item():.4f}')\n        print(f'Discriminator (Critic) loss: {loss_critic.item():.4f}')\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results and Analysis\n### Hyperparameter and Model Architecture Comparison Discussion\nAs mentioned previously, I used CycleGAN to generate Monet-style paintings from the real photos dataset. The Generator consists of an initial convolutional layer, downsample layers, residual blocks, upsample layers, and an output layer. Initial Convolution Layer: Applies convolution with a large kernel size (7x7) to the input image. Downsample Layers: These layers downsample the input image. Residual Blocks: These blocks preserve the identity of the input image. Upsample Layers: These layers upsample the image back to the original size. Output Layer: Applies a convolutional layer with a tanh activation function to produce the final generated image. The Discriminator consists of convolutional layers followed by LeakyReLU activations. These layers aim to distinguish between real images and those generated by the generator.\n\nSome hyperparameters that I chose to tweak on my submission run were critic iterations, weight clip, lambda identity, identity loss function, numner of epochs, and learning rate. Critic iterations are the number of iterations to train the critic for each generator iteration. It sets the number of times the discriminator is trained before the generator in order to update the discriminator weights. Weight clip is the value to satisfy the Lipschitz constraint. Lambda identity weight for the identity loss. Identity loss function is the loss function used to calculate the identity loss. Learning rate is used for the RMSprop optimizer.\n\n## Conclusions\n### Results discussion\nOnce my model was submitted, I was able to obtain a score of 91.01% accuracy. ","metadata":{}},{"cell_type":"markdown","source":"## Code to Generate Submission File","metadata":{}},{"cell_type":"code","source":"# create path to save created monet paintings\ntransformed_save_dir = '../images'\nif not os.path.exists(transformed_save_dir):\n    os.makedirs(transformed_save_dir)\n    \n# freeze some network params to generate imgs\nG.eval()\n\n\nfor i, real_photos in enumerate(photo_loader):\n    real_photos = real_photos.to(device)\n    with torch.no_grad():\n        monet_style_imgs = G(real_photos)\n\n    # Save each transformed image\n    for j, img in enumerate(monet_style_imgs):\n        save_path = os.path.join(transformed_save_dir, f'monet_painting_{i * batch_size + j}.png')\n        torchvision.utils.save_image(img, save_path)\n\nprint(f\"Transformed images are saved in {transformed_save_dir}\")\n\nshutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")","metadata":{},"execution_count":null,"outputs":[]}]}